{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example demonstrates how to marginalize out discrete assignment variables\n",
    "in a Pyro model.\n",
    "\n",
    "Our example model is Latent Dirichlet Allocation. While the model in this\n",
    "example does work, it is not the recommended way of coding up LDA in Pyro.\n",
    "Whereas the model in this example treats documents as vectors of categorical\n",
    "variables (vectors of word ids), it is usually more efficient to treat\n",
    "documents as bags of words (histograms of word counts).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import functools\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "logging.basicConfig(format='%(relativeCreated) 9d %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a fully generative model of a batch of documents.\n",
    "# data is a [num_words_per_doc, num_documents] shaped array of word ids\n",
    "# (specifically it is not a histogram). We assume in this simple example\n",
    "# that all documents have the same number of words.\n",
    "def model(data=None, args=None, batch_size=None):\n",
    "    # Globals.\n",
    "    with pyro.plate(\"topics\", args.num_topics):  # 8\n",
    "        # sample a topic weight (alpha?)\n",
    "        # (8)\n",
    "        topic_weights = pyro.sample(\"topic_weights\", dist.Gamma(1. / args.num_topics, 1.))\n",
    "        # sample prob of words for each topic (beta?)\n",
    "        # symmetric Dirichlet distribution - alpha is the same for each word\n",
    "        # (8 x 1024) (num_topics x num_words), sum in each row = 1\n",
    "        topic_words = pyro.sample(\"topic_words\",\n",
    "                                  dist.Dirichlet(torch.ones(args.num_words) / args.num_words))\n",
    "\n",
    "    # Locals.\n",
    "    with pyro.plate(\"documents\", args.num_docs) as ind:  # 1000\n",
    "        if data is not None:\n",
    "            # PyTorch jit compiler in Pyro models - for speeding up models\n",
    "            # ignore warnings in safe code blocks\n",
    "            with pyro.util.ignore_jit_warnings():\n",
    "                assert data.shape == (args.num_words_per_doc, args.num_docs)\n",
    "            # indeksy???\n",
    "            data = data[:, ind]\n",
    "        # documents vs topics - dirichlet dist, alpha as input\n",
    "        # (1000 x 8) (num-docs x num_topics), sum in each row = 1\n",
    "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(topic_weights))\n",
    "        # for every word in doc\n",
    "        with pyro.plate(\"words\", args.num_words_per_doc):  # 64\n",
    "            # The word_topics variable is marginalized out during inference,\n",
    "            # achieved by specifying infer={\"enumerate\": \"parallel\"} and using\n",
    "            # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n",
    "            # the guide.\n",
    "            \n",
    "            # samples one topic from 8 available according to doc_topics probs\n",
    "            # word_topics ~ (64 x 1000) (num_words_per_doc x num_docs)\n",
    "            word_topics = pyro.sample(\"word_topics\", dist.Categorical(doc_topics),\n",
    "                                      infer={\"enumerate\": \"parallel\"})\n",
    "            # samples 64 words per doc\n",
    "            # topic_words[word_topics] ~ (8 x 64 x 1000 x 1024)\n",
    "            # ~ (num_topics x num_word_per_doc x num_docs x num_words)\n",
    "            data = pyro.sample(\"doc_words\", dist.Categorical(topic_words[word_topics]),\n",
    "                               obs=data)\n",
    "            # data ~ (64 x 1000) (num_words_per_doc x num_docs)\n",
    "\n",
    "    return topic_weights, topic_words, data\n",
    "\n",
    "\n",
    "# We will use amortized inference of the local topic variables, achieved by a\n",
    "# multi-layer perceptron. We'll wrap the guide in an nn.Module.\n",
    "def make_predictor(args):\n",
    "    layer_sizes = ([args.num_words] +\n",
    "                   [int(s) for s in args.layer_sizes.split('-')] +\n",
    "                   [args.num_topics])\n",
    "    logging.info('Creating MLP with sizes {}'.format(layer_sizes))\n",
    "    layers = []\n",
    "    for in_size, out_size in zip(layer_sizes, layer_sizes[1:]):\n",
    "        layer = nn.Linear(in_size, out_size)\n",
    "        layer.weight.data.normal_(0, 0.001)\n",
    "        layer.bias.data.normal_(0, 0.001)\n",
    "        layers.append(layer)\n",
    "        layers.append(nn.Sigmoid())\n",
    "    layers.append(nn.Softmax(dim=-1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def parametrized_guide(predictor, data, args, batch_size=None):\n",
    "    # Use a conjugate guide for global variables.\n",
    "    topic_weights_posterior = pyro.param(\n",
    "            \"topic_weights_posterior\",\n",
    "            lambda: torch.ones(args.num_topics),\n",
    "            constraint=constraints.positive)\n",
    "    topic_words_posterior = pyro.param(\n",
    "            \"topic_words_posterior\",\n",
    "            lambda: torch.ones(args.num_topics, args.num_words),\n",
    "            constraint=constraints.greater_than(0.5)) # ??\n",
    "    with pyro.plate(\"topics\", args.num_topics):\n",
    "        pyro.sample(\"topic_weights\", dist.Gamma(topic_weights_posterior, 1.))\n",
    "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
    "\n",
    "    # Use an amortized guide for local variables.\n",
    "    pyro.module(\"predictor\", predictor)\n",
    "    # \n",
    "    with pyro.plate(\"documents\", args.num_docs, batch_size) as ind:\n",
    "        # The neural network will operate on histograms rather than word\n",
    "        # index vectors, so we'll convert the raw data to a histogram.\n",
    "        if torch._C._get_tracing_state():\n",
    "            # ones on the diagonal and zeros elsewhere\n",
    "            counts = torch.eye(1024)[data[:, ind]].sum(0).t()\n",
    "        else:\n",
    "            counts = torch.zeros(args.num_words, ind.size(0))\n",
    "            # https://pytorch.org/docs/stable/tensors.html\n",
    "            # scatter_add_(dim, index, other)\n",
    "            # Adds all values from the tensor other into self at the indices specified in the index tensor \n",
    "            # in a similar fashion as scatter_(). For each value in other, it is added to an index in self \n",
    "            # which is specified by its index in other for dimension != dim and by the corresponding value \n",
    "            # in index for dimension = dim.\n",
    "            # self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0\n",
    "            counts.scatter_add_(0, data[:, ind], torch.tensor(1.).expand(counts.shape))\n",
    "            \n",
    "        doc_topics = predictor(counts.transpose(0, 1))\n",
    "        \n",
    "        # a single point (dirac delta), event_dim - event dimension\n",
    "        pyro.sample(\"doc_topics\", dist.Delta(doc_topics, event_dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    logging.info('Generating data')\n",
    "    pyro.set_rng_seed(0)\n",
    "    pyro.clear_param_store()\n",
    "    pyro.enable_validation(True)\n",
    "\n",
    "    # We can generate synthetic data directly by calling the model.\n",
    "    true_topic_weights, true_topic_words, data = model(args=args)\n",
    "\n",
    "    # We'll train using SVI.\n",
    "    logging.info('-' * 40)\n",
    "    logging.info('Training on {} documents'.format(args.num_docs))\n",
    "    predictor = make_predictor(args)\n",
    "    guide = functools.partial(parametrized_guide, predictor)\n",
    "    Elbo = JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO\n",
    "    elbo = Elbo(max_plate_nesting=2)\n",
    "    optim = Adam({'lr': args.learning_rate})\n",
    "    svi = SVI(model, guide, optim, elbo)\n",
    "    logging.info('Step\\tLoss')\n",
    "    for step in range(args.num_steps):\n",
    "        loss = svi.step(data, args=args, batch_size=args.batch_size)\n",
    "        if step % 10 == 0:\n",
    "            logging.info('{: >5d}\\t{}'.format(step, loss))\n",
    "    loss = elbo.loss(model, guide, data, args=args)\n",
    "    logging.info('final loss = {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   131822 Generating data\n",
      "   138031 ----------------------------------------\n",
      "   138031 Training on 1000 documents\n",
      "   138031 Creating MLP with sizes [1024, 100, 100, 8]\n",
      "   138074 Step\tLoss\n",
      "   139223     0\t483160.40625\n",
      "   143525    10\t470484.21875\n",
      "   147795    20\t483758.96875\n",
      "   152067    30\t485707.21875\n",
      "   156332    40\t486927.15625\n",
      "   160520    50\t477152.90625\n",
      "   164781    60\t483504.375\n",
      "   168987    70\t495452.59375\n",
      "   173187    80\t491391.25\n",
      "   177369    90\t474378.1875\n",
      "   181647   100\t484284.96875\n",
      "   185867   110\t494794.75\n",
      "   190025   120\t480864.96875\n",
      "   194189   130\t480555.5625\n",
      "   198492   140\t472492.78125\n",
      "   202747   150\t472694.6875\n",
      "   206918   160\t465221.96875\n",
      "   211180   170\t494591.71875\n",
      "   215430   180\t486451.53125\n",
      "   219728   190\t475809.46875\n",
      "   223996   200\t468522.40625\n",
      "   228214   210\t472827.09375\n",
      "   232435   220\t488274.09375\n",
      "   236652   230\t478799.8125\n",
      "   240975   240\t477782.0\n",
      "   245221   250\t487383.4375\n",
      "   249465   260\t466331.6875\n",
      "   253729   270\t457770.96875\n",
      "   257984   280\t461084.5\n",
      "   262284   290\t478705.5\n",
      "   266584   300\t465831.25\n",
      "   270865   310\t476572.0625\n",
      "   275085   320\t471013.34375\n",
      "   279456   330\t476240.625\n",
      "   283688   340\t477306.0\n",
      "   287953   350\t475412.03125\n",
      "   292199   360\t470567.84375\n",
      "   296443   370\t473307.625\n",
      "   300730   380\t457488.34375\n",
      "   305043   390\t451830.59375\n",
      "   309326   400\t457315.09375\n",
      "   313725   410\t479201.0\n",
      "   317995   420\t467236.46875\n",
      "   322237   430\t454401.6875\n",
      "   326550   440\t455277.875\n",
      "   330787   450\t471193.3125\n",
      "   335045   460\t460399.5\n",
      "   339270   470\t461490.5\n",
      "   343701   480\t458611.5\n",
      "   347963   490\t465937.03125\n",
      "   352258   500\t455670.5625\n",
      "   356558   510\t470369.0\n",
      "   360859   520\t462575.4375\n",
      "   365134   530\t442589.9375\n",
      "   369378   540\t459670.125\n",
      "   373647   550\t442267.9375\n",
      "   377930   560\t456340.125\n",
      "   382220   570\t444939.15625\n",
      "   386500   580\t444772.6875\n",
      "   390786   590\t464580.0625\n",
      "   395014   600\t449504.15625\n",
      "   399306   610\t451422.75\n",
      "   403545   620\t451156.15625\n",
      "   407792   630\t451143.0\n",
      "   412028   640\t447823.53125\n",
      "   416372   650\t460403.71875\n",
      "   420678   660\t457964.5\n",
      "   424913   670\t460151.625\n",
      "   429294   680\t437320.84375\n",
      "   433570   690\t461952.875\n",
      "   437849   700\t440956.8125\n",
      "   442050   710\t441823.71875\n",
      "   446323   720\t448019.1875\n",
      "   450629   730\t443259.09375\n",
      "   454874   740\t444948.40625\n",
      "   459094   750\t451391.34375\n",
      "   463355   760\t458087.40625\n",
      "   467597   770\t458756.5\n",
      "   471855   780\t446977.5\n",
      "   476137   790\t426915.65625\n",
      "   480479   800\t449277.59375\n",
      "   484782   810\t437221.75\n",
      "   489058   820\t439863.46875\n",
      "   493313   830\t433459.25\n",
      "   497637   840\t433608.0\n",
      "   501903   850\t438257.625\n",
      "   506228   860\t434768.6875\n",
      "   510575   870\t446322.03125\n",
      "   514978   880\t440008.6875\n",
      "   519288   890\t431745.34375\n",
      "   523555   900\t430724.59375\n",
      "   527791   910\t443041.21875\n",
      "   532026   920\t439877.5625\n",
      "   536372   930\t416066.84375\n",
      "   540620   940\t443692.375\n",
      "   544961   950\t425688.9375\n",
      "   549277   960\t434426.59375\n",
      "   553544   970\t437918.15625\n",
      "   557839   980\t435585.6875\n",
      "   562140   990\t421044.25\n",
      "   568733 final loss = 426838.625\n"
     ]
    }
   ],
   "source": [
    "# num_topics = 8\n",
    "# num_words = 1024\n",
    "# num_docs = 1000\n",
    "# num_words_per_doc = 64\n",
    "# num_steps = 1000\n",
    "# layer_sizes = \"100-100\"\n",
    "# learning_rate = 0.001\n",
    "# batch_size = 32\n",
    "# jit = 'store_true'\n",
    "# %%time\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Amortized Latent Dirichlet Allocation\")\n",
    "parser.add_argument(\"-t\", \"--num-topics\", default=8, type=int)\n",
    "parser.add_argument(\"-w\", \"--num-words\", default=1024, type=int)\n",
    "parser.add_argument(\"-d\", \"--num-docs\", default=1000, type=int)\n",
    "parser.add_argument(\"-wd\", \"--num-words-per-doc\", default=64, type=int)\n",
    "parser.add_argument(\"-n\", \"--num-steps\", default=1000, type=int)\n",
    "parser.add_argument(\"-l\", \"--layer-sizes\", default=\"100-100\")\n",
    "parser.add_argument(\"-lr\", \"--learning-rate\", default=0.001, type=float)\n",
    "parser.add_argument(\"-b\", \"--batch-size\", default=32, type=int)\n",
    "parser.add_argument('--jit', action='store_true')\n",
    "args = parser.parse_args(\"-t 8\".split())\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
